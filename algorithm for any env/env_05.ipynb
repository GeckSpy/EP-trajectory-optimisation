{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating gymnasium environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to adapt this notebook to your environment ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "DQN :\n",
    "- The input size of the nn depends on the size of the input state : adapt the size of the first layer \n",
    "\n",
    "Env: \n",
    "- Don't modify :\n",
    "    - policy \n",
    "    - random_action \n",
    "\n",
    "- Modify :\n",
    "    - init :\n",
    "        - n_action : we assume the action space to be finite, and we hope small \n",
    "        - done : this attribute say if the episode is finished, it needs to be set to true when we do the reset, and we need to update in the function step\n",
    "        - every attribute you need to build the function step, state, show_state\n",
    "    - state : must return a batch of size 1 : \n",
    "        - typically our state is a vector of size say d, so a tensor of size (d). Here we want the function \n",
    "            to return a batch of size one , so we return a vector of size (1,d) e.g [t] where t is our size \n",
    "    - show_state \n",
    "    - reset : must update the attribute done, and your internal attribute that give the current state of the episode\n",
    "    - step : take an action as a tensor (we access it by action.item() ) and returns a namedtuple Transition \n",
    "    with 4 coordinates (state,action,next_state,reward) \n",
    "            - every reward must be in [0,1] to make easier the hyperparameter finetuning\n",
    "            - state and next_state must be a batch of size one (as for the function state)\n",
    "            - action should be a batch of size one\n",
    "            - if the episode is finished after the action, next_state must be None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is:\n",
    " - A $N \\times M$ grid of cases with $N,M \\in \\mathbb{N}$ supposed to be a loop track.\n",
    " - Each case is in $\\{0,1,3,4\\}$ such that:\n",
    "   - $0 \\rightarrow$ road (white)\n",
    "   - $1 \\rightarrow$ wall (black)\n",
    "   - $STAR\\_ CHAR$ (3) $\\rightarrow$ starting point\n",
    "\n",
    " - A Car that follow some model for trajectory. The car can do the next action:\n",
    "   - accelerate (add $1$ to car speed)\n",
    "   - brake (divide the car speed by $1.5$)\n",
    "   - turn (need to specify the angle)\n",
    "   - do nothing\n",
    "  \n",
    " - Possible Actions $(a,b) \\in \\{-1,0,1\\} \\times [-K,K]$ where:\n",
    "   - $a=1$ for acceleration, $-1$ for brake and $0$ for nothing\n",
    "   - $b>0$ for turning left with angle, $b<0$ to turn right and 0 for nothing. Note that a turn, in this environment is always in $[-K,K]$ where $K$ is the value of `env.max_turn` variable.\n",
    "\n",
    " - The state is a $8$ vector $V$\n",
    "   - $V[0]$ is the speed of the car $\\geq 0$\n",
    "   - $V[i]$ (for $i \\in [8]$) is the distance to the wall with rotation $90 - 30i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "To create your turn/your track, use the grid notation (for better visualisation) like env_01 and env_02. The grid is used with the matrix notation. If we denote the grid by $G$, then $G[0][0]$ is the top left case, $G[x][y]$ is the case on the $x$-th row (from top to bottom) and on the $y$-th column (from left to right).\n",
    "\n",
    "You can also use image for that with balck pixel for wall, white for starting point, any other color will be interpreted as road.\n",
    "\n",
    "Nevertheless, for all the other things, we use axis coordinates, i.e., $G[x][y]$ refer to the case situate at coordinate $(x,y)$. More preciselly, it is the case on the $x$-th column (from left to right) and on the $y$-th row (from bottom to top). Therefore you can simply use `plt.plot(x,y, args**)` for ploting something at coordinate $(x,y)$ (see code for more example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import random as rd\n",
    "import time\n",
    "\n",
    "from gym import Env, spaces\n",
    "\n",
    "# For saving files\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "### Coor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coor():\n",
    "    def __init__(self, coor):\n",
    "        self.x = coor[0]\n",
    "        self.y = coor[1]\n",
    "\n",
    "    def get(self):\n",
    "        return self.x, self.y\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"(\" + str(self.x) + \", \" + str(self.y) + \")\"\n",
    "    \n",
    "    def __add__(self, coor2):\n",
    "        return Coor((self.x + coor2.x, self.y + coor2.y))\n",
    "    \n",
    "    def __eq__(self, coor2):\n",
    "        if coor2 == None:\n",
    "            return False\n",
    "        return (self.x==coor2.x) and (self.y==coor2.y)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        x,y = self.get()\n",
    "        return Coor((-x,-y))\n",
    "    \n",
    "    def __sub__(self, coor2):\n",
    "        coor = - coor2\n",
    "        return self + coor\n",
    "    \n",
    "    def norm(self):\n",
    "        x,y = self.get()\n",
    "        return np.sqrt(x*x + y*y)\n",
    "    \n",
    "    def dist(self, coor2):\n",
    "        return (self -coor2).norm()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to previous environment, track are not matrix anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RED = [255, 0, 0]\n",
    "GREEN = [0, 255, 0]\n",
    "BLUE = [0, 0, 255]\n",
    "GREY = [70 for _ in range(3)]\n",
    "WHITE = [240 for _ in range(3)]\n",
    "\n",
    "START_CHAR = 2\n",
    "CAR_CHAR = 4\n",
    "\n",
    "def color_track(b):\n",
    "    if b == START_CHAR:\n",
    "        return GREEN\n",
    "    elif b == 1:\n",
    "        return GREY\n",
    "    else:\n",
    "        return WHITE\n",
    "    \n",
    "\n",
    "class Track():\n",
    "    def __init__(self, tab):\n",
    "        #switching height and width for plan approach\n",
    "        self.height, self.width = np.array(tab).shape\n",
    "        self.basic_info_track:list = np.array(tab)\n",
    "        \n",
    "        self.info_track:list = [[0 for _ in range(self.height)] for _ in range(self.width)]\n",
    "        for x in range(self.width):\n",
    "            for y in range(self.height):\n",
    "                self.info_track[x][y] = self.basic_info_track[self.height-1-y][x]\n",
    "\n",
    "        self.color_track = [[color_track(self.info_track[x][y]) for x in range(self.width)] for y in range(self.height)]\n",
    "\n",
    "        self.start = None\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                if self.info_track[x][y] == START_CHAR:\n",
    "                    self.start = Coor((x,y))\n",
    "\n",
    "    def get_color(self, coor:Coor):\n",
    "        \"\"\"return the color of the case x,y\"\"\"\n",
    "        x,y = coor.get()\n",
    "        return color_track(self.info_track[x][y])\n",
    "    \n",
    "    def is_wall(self, coor:Coor):\n",
    "        \"\"\"Return True if case (x,y) is a wall\"\"\"\n",
    "        x,y = coor.get()\n",
    "        nx,ny = int(round(x)), int(round(y))\n",
    "        return (self.info_track[nx][ny] == 1)\n",
    "\n",
    "    def get_start(self):\n",
    "        \"\"\"Return coordinate of start\"\"\"\n",
    "        if self.start == None:\n",
    "            return None\n",
    "        return self.start.get()\n",
    "    \n",
    "    def get_end(self):\n",
    "        \"\"\"Return coordinate of end\"\"\"\n",
    "        return self.end.get()\n",
    "    \n",
    "    def is_case_ridable(self, coor: Coor):\n",
    "        \"\"\"Return if the car can go on the coordinate or not\"\"\"\n",
    "        x,y = coor.get()\n",
    "        x,y = int(round(x)), int(round(y))\n",
    "        if not (x>=0 and x<self.width and y>=0 and y<self.height):\n",
    "            return False\n",
    "        return not self.is_wall(coor)\n",
    "    \n",
    "    def is_move_possible(self, a:Coor, b:Coor) -> bool:\n",
    "        \"\"\"Return if the car can go from point a to b in straight line\"\"\"\n",
    "        diff_x = b.x-a.x\n",
    "        diff_y = b.y-a.y\n",
    "\n",
    "        d = a.dist(b)\n",
    "        if d<1:\n",
    "            d = 1\n",
    "        \n",
    "        space = np.arange(0, 1, 1/d)\n",
    "        for t in space:\n",
    "            case = Coor((a.x+t*diff_x, a.y+t*diff_y))\n",
    "            if not self.is_case_ridable(case):\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def next_wall(self, coor:Coor, alpha:float, dist_max=None):\n",
    "        \"\"\"Return the next in the line from coor to the first wall\"\"\"\n",
    "        alpha = alpha % 360\n",
    "        dx = np.cos(alpha * np.pi/180)\n",
    "        dy = np.sin(alpha * np.pi/180)\n",
    "\n",
    "        i = 0\n",
    "        next_coor = Coor( (int(round(coor.x + i*dx)), int(round(coor.y + i*dy))) )\n",
    "        while self.is_case_ridable(next_coor):\n",
    "            if (dist_max!=None) and (coor.dist(next_coor) > dist_max):\n",
    "                break\n",
    "            i += 1\n",
    "            next_coor  = Coor( (int(round(coor.x + i*dx)), int(round(coor.y + i*dy))) )\n",
    "        return next_coor\n",
    "\n",
    "    def plot(self, hide=False):\n",
    "        \"\"\"Plot the track using matplotlib\"\"\"\n",
    "        plt.imshow(self.color_track, origin='lower')\n",
    "        plt.axis(\"off\")\n",
    "        if not hide:\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../tracks/03.png\"\n",
    "\n",
    "from matplotlib.image import imread\n",
    "from PIL import Image\n",
    "\n",
    "def info_from_real_color(tab):\n",
    "    x,y,z = tab[0], tab[1], tab[2]\n",
    "    if x==0 and y==0 and z==0:\n",
    "        return 1\n",
    "    elif x==255 and y==255 and z==255:\n",
    "        return START_CHAR\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def create_track_info(path):\n",
    "    img = Image.open(path)\n",
    "    arr = np.array(img)\n",
    "    img.close()\n",
    "    return [[info_from_real_color(y) for y in x] for x in arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Car class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Car has $2$ main variable:\n",
    " - speed: $v$\n",
    " - alpha: $\\alpha$ suppose to be between $0$ and $360$\n",
    "\n",
    "To calculate the $x$-speed and $y$-speed we use the next formulas:\n",
    " - $v_x = v.cos(\\alpha . \\frac{\\pi}{180})$\n",
    " - $v_y = v.sin(\\alpha . \\frac{\\pi}{180})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Constant\"\"\"\n",
    "MAX_SPEED = 50\n",
    "MAX_TURN = 20\n",
    "\n",
    "\"\"\"Class\"\"\"\n",
    "class Car():\n",
    "    def __init__(self, coor:Coor):\n",
    "        self.coor: Coor = Coor((coor.x, coor.y))\n",
    "        self.speed: float = 0\n",
    "        self.alpha: float = 0 # The angle of the car according to unitary cicrle\n",
    "        self.trajectory = [[Coor((coor.x, coor.y)), 0]]\n",
    "        self.previous_speed: float = 0\n",
    "        self.time = 1\n",
    "\n",
    "        self.max_turn = MAX_TURN\n",
    "        self.max_speed = MAX_SPEED\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"C[\" + str(self.coor) + \" \" + str(self.speed) + \" \" + str(self.alpha) + \"]\"\n",
    "    \n",
    "\n",
    "    def accelerate(self, amont=1):\n",
    "        \"\"\"Increase speed of the car\"\"\"\n",
    "        acceleration_constant = 1\n",
    "        self.speed += acceleration_constant*amont * self.time\n",
    "        self.speed = min(self.speed, self.max_speed)\n",
    "        \n",
    "\n",
    "    def brake(self, amont=1):\n",
    "        \"\"\"Decrease speed of the car (can't drive backward)\"\"\"\n",
    "        brake_constant = 2\n",
    "        self.speed -= brake_constant*amont*self.time\n",
    "        if self.speed < 0:\n",
    "            self.speed = 0\n",
    "\n",
    "    def turn(self, deg):\n",
    "        \"\"\"Change the current rotation of the car\"\"\"\n",
    "        if np.absolute(deg) > self.max_turn:\n",
    "            print(deg)\n",
    "            assert False\n",
    "        self.alpha += deg\n",
    "        self.alpha = self.alpha % 360\n",
    "\n",
    "    def get_speed_coor(self):\n",
    "        cst: float = np.pi / 180\n",
    "        dx: float = self.speed * np.cos(self.alpha * cst)\n",
    "        dy: float = self.speed * np.sin(self.alpha * cst)\n",
    "        return Coor((dx,dy))\n",
    "\n",
    "    def move(self):\n",
    "        \"\"\"Change the coordinate of the care according to its speed and alpha\"\"\"\n",
    "        speed_increase = 0\n",
    "        if self.previous_speed < self.speed:\n",
    "            speed_increase = 1\n",
    "        elif self.previous_speed > self.speed:\n",
    "            speed_increase = -1\n",
    "        self.previous_speed = self.speed\n",
    "\n",
    "        dx,dy = self.get_speed_coor().get()\n",
    "        self.coor.x += dx\n",
    "        self.coor.y += dy\n",
    "        self.trajectory.append([Coor((self.coor.x, self.coor.y)), speed_increase])\n",
    "\n",
    "    def dic(self):\n",
    "        return {\"coor\":self.coor, \"speed\":self.speed, \"alpha\":self.alpha, \"trajectory\":self.trajectory}\n",
    "\n",
    "    def plot(self, markersize=8, vector_constant=2, show_trajectory=False, head_width=1):\n",
    "        \"\"\"Plot the car and is speed vectors\"\"\"\n",
    "        # Plot car\n",
    "        x,y = self.coor.get()\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Plot \n",
    "        if show_trajectory:\n",
    "            liste_x = [i[0].x for i in self.trajectory]\n",
    "            liste_y = [i[0].y for i in self.trajectory]\n",
    "\n",
    "            for i in range(1, len(self.trajectory)):\n",
    "                color = \"yellow\"\n",
    "                if self.trajectory[i][1] == 1:\n",
    "                    color = \"green\"\n",
    "                elif self.trajectory[i][1] == -1:\n",
    "                    color = \"red\"\n",
    "        \n",
    "                plt.plot([liste_x[i-1], liste_x[i]], [liste_y[i-1], liste_y[i]], \"-o\", color=color, markersize=2)\n",
    "                 \n",
    "        # Plot car's directoin\n",
    "        cst: float = np.pi / 180\n",
    "        dx: float = np.cos(self.alpha * cst)\n",
    "        dy: float = np.sin(self.alpha * cst)\n",
    "        plt.arrow(x, y, dx/10, dy/10, head_width=head_width)\n",
    "        plt.plot([x, x+ dx*self.speed*vector_constant], [y, y+ dy*self.speed*vector_constant], \"-\", color=\"red\")\n",
    "        plt.plot(x, y, \"o\", color='blue', markersize=markersize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gym env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform a actual action $(a, \\alpha) \\in \\{-1,0,1\\} \\times \\{-K,...,K\\}$ where $K =$ `MAX_TURN` to a action in $\\{0,...,N-1\\}$.\n",
    "\n",
    "Then we need a bijection\n",
    "$$\\phi : \\{-1,0,1\\} \\times \\{-K,...,K\\} \\rightarrow \\mathbb{Z}_N$$\n",
    "\n",
    "We can take\n",
    "$$\\phi(a,b) = 3(b+K) + (a+1)$$\n",
    "\n",
    "Then,\n",
    "$$\\phi^{-1}(x) = ((x\\; mod\\; 3)-1, \\frac{1}{3}x -K)$$\n",
    "\n",
    "Moreover, we have $N = 3\\times 2K+1 = 6K+3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SPEED = 50\n",
    "MAX_TURN = 20\n",
    "\n",
    "class RacingCar(Env):\n",
    "    def __init__(self, track_path):\n",
    "        super(RacingCar, self).__init__()\n",
    "        # time between two frames\n",
    "        self.time = 1 #Change this variable to \"discretiser\" the time. Lower value means more discretisation\n",
    "\n",
    "        self.max_turn = int(MAX_TURN * self.time)\n",
    "        self.nb_state = 6*self.max_turn + 3\n",
    "        self.max_speed = int(MAX_SPEED * self.time)\n",
    "\n",
    "        # Define an action space ranging from 0 to 3\n",
    "        self.action_space = [self.int_to_action(i) for i in range(self.nb_state)]\n",
    "        self.int_action_space = [i for i in range(self.nb_state)]\n",
    "\n",
    "        self.track_info = create_track_info(track_path)\n",
    "        self.track: Track = Track(self.track_info)\n",
    "\n",
    "        # Define the anle of which we will look the distance\n",
    "        self.liste_alpha = [90, 60, 30, 0, -30, -60, -90]\n",
    "        self.max_dist_wall = self.track.height + self.track.width\n",
    "        \n",
    "        self.car: Car = self.create_car()\n",
    "\n",
    "        self.canvas = np.array(self.track.color_track)\n",
    "\n",
    "    def create_car(self):\n",
    "        car = Car(self.track.start)\n",
    "        car.max_turn = self.max_turn\n",
    "        car.max_speed = self.max_speed\n",
    "        car.time = self.time\n",
    "        return car\n",
    "\n",
    "    def action_to_int(self, action):\n",
    "        \"\"\"Transform an action (tuple) into an action (int)\"\"\"\n",
    "        a,b = action\n",
    "        return 3*(b+self.max_turn) + a+1\n",
    "    \n",
    "    def int_to_action(self, x):\n",
    "        \"\"\"Transform an action (int) into an action (tuple)\"\"\"\n",
    "        return ((x%3)-1, int(x/3) -self.max_turn)\n",
    "\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"Return actual state of the env\"\"\"\n",
    "        state = [self.car.speed]\n",
    "        for alpha in self.liste_alpha:\n",
    "            coor = self.track.next_wall(self.car.coor, self.car.alpha + alpha, dist_max=self.max_dist_wall)\n",
    "            state.append(self.car.coor.dist(coor))\n",
    "        return state\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment\"\"\"\n",
    "        self.car = self.create_car()\n",
    "        return self.get_state(), []\n",
    "\n",
    "    def render(self, waiting_time=0.01, show_trajectory=False, show_dist_to_wall=False):\n",
    "        \"\"\"Render the environment\"\"\"\n",
    "        self.track.plot(hide=True)\n",
    "        if show_dist_to_wall:\n",
    "            for alpha in self.liste_alpha:\n",
    "                coor = self.track.next_wall(self.car.coor, self.car.alpha + alpha, dist_max=self.max_dist_wall)\n",
    "                plt.plot([self.car.coor.x, coor.x], [self.car.coor.y, coor.y], \"-\", color=\"grey\")\n",
    "\n",
    "        self.car.plot(show_trajectory=show_trajectory)\n",
    "        display.clear_output(wait=True)\n",
    "        plt.show()\n",
    "        time.sleep(waiting_time)\n",
    "        \n",
    "    def step(self, action:int):\n",
    "        \"\"\"Do a step, we suppose that the action is a possible one\"\"\"\n",
    "        is_done = False\n",
    "        reward = 0\n",
    "\n",
    "        x,y = self.car.coor.get()\n",
    "        previous_coor = Coor((x,y))\n",
    "\n",
    "        acc, turn = self.int_to_action(action)\n",
    "        if acc==-1:\n",
    "            self.car.brake()\n",
    "        elif acc==1:\n",
    "            self.car.accelerate()\n",
    "        self.car.turn(turn)\n",
    "        self.car.move()\n",
    "\n",
    "        new_coor = self.car.coor\n",
    "        reward += previous_coor.dist(new_coor)\n",
    "\n",
    "        has_crashed = False\n",
    "        if not self.track.is_move_possible(previous_coor, new_coor):\n",
    "            has_crashed = True\n",
    "            reward = -100\n",
    "            is_done = True\n",
    "\n",
    "        return self.get_state(), reward, is_done, has_crashed, []\n",
    "    \n",
    "    def random_action(self, p_accel=0.25, p_brake=0.25, p_turn=0.5):\n",
    "        \"\"\"Return random possible action according to probability\"\"\"\n",
    "        action = [0,0]\n",
    "        rd_accel = rd.random()\n",
    "        if rd_accel <= p_accel:\n",
    "            action[0] = 1\n",
    "        elif rd_accel <= p_accel + p_brake:\n",
    "            action[0] = -1\n",
    "        \n",
    "        if rd.random() <= p_turn:\n",
    "            action[1] = ((-1)**(rd.randint(0,1))) * rd.randint(-self.max_turn, self.max_turn)\n",
    "        return tuple(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Env example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Env = RacingCar(path)\n",
    "print(Env.nb_state)\n",
    "print(Env.action_space)\n",
    "for i in range(Env.nb_state):\n",
    "    acc,turn = Env.int_to_action(i)\n",
    "    if (np.absolute(turn) > Env.max_turn) or (np.absolute(Env.action_space[i][1]) > Env.max_turn):\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moves = [[Coor((60, 25)), Coor((100,28))],\n",
    "         [Coor((125, 80)), Coor((105, 100))],\n",
    "         [Coor((300,120)), Coor((290, 100))],\n",
    "         [Coor((230,120)), Coor((210, 80))],\n",
    "         [Coor((363.5,140)), Coor((359.2, 260))],\n",
    "         [Coor((230, 230)), Coor((230, 230))],\n",
    "         [Coor((290, 200)), Coor((290, 200))]]\n",
    "\n",
    "for coors in moves:\n",
    "    res = Env.track.is_move_possible(coors[0], coors[1])\n",
    "    color = \"limegreen\"\n",
    "    if not res:\n",
    "        color = \"red\"\n",
    "    plt.plot([coors[0].x, coors[1].x], [coors[0].y, coors[1].y], \"-o\", markersize=4, color=color)\n",
    "\n",
    "Env.track.plot(hide=True)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Env.render(show_dist_to_wall=True, show_trajectory=True)\n",
    "\n",
    "print(Env.time, Env.max_turn)\n",
    "print(Env.car.time, Env.car.max_turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Env.reset()\n",
    "\n",
    "actions = [(1,-10), (1, -10), (1,0), (1,0), (1,10), (1, 10), (1,0), (1,0), (1,0), (1,0), (1,0), (1,0), (1,0), (1,0), (1,0), (1,0), (1,0),\n",
    "           (1,0), (-1,0), (-1,0), (-1,0), (-1, Env.max_turn), (0, Env.max_turn), (0, Env.max_turn), (0, Env.max_turn), (1,0), (1,0), (1,0)]\n",
    "\n",
    "for action in actions:\n",
    "    if np.absolute(action[1]) <= Env.max_turn:\n",
    "        Env.step(Env.action_to_int(action))\n",
    "        Env.render(show_trajectory=True, show_dist_to_wall=True)\n",
    "    \n",
    "print(Env.get_state())\n",
    "Env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Env problem testing (solved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../tracks/02.png\"\n",
    "env2 = RacingCar(path)\n",
    "env2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# structure to save transitions \n",
    "from collections import namedtuple , deque\n",
    "Transition = namedtuple(\"Transition\",[\"state\",\"action\",\"next_state\",\"reward\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReplayMem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self,maxlen : int):\n",
    "        self.memory_ = deque(maxlen=maxlen)\n",
    "\n",
    "    def push(self,x : Transition):\n",
    "        self.memory_.append(x)\n",
    "\n",
    "    def sample(self,batch_size : int) -> list[Transition]:\n",
    "        return rd.sample(self.memory_,batch_size)\n",
    "    \n",
    "    def clear(self):\n",
    "        return self.memory_.clear()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self,layer_size,state_size,action_n):\n",
    "        super(DQN,self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_size,layer_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(layer_size,layer_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(layer_size,layer_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(layer_size,action_n),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def save(self,filename : str = None):\n",
    "        if (filename == None):\n",
    "            filename = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "        torch.save(self.state_dict(),filename)\n",
    "\n",
    "    def load(self,filename : str):\n",
    "        self.load_state_dict(torch.load(filename, weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self, track_name):\n",
    "        \"\"\" track_name is the name of the track file\"\"\"\n",
    "        # Toujours là\n",
    "        self.done = False\n",
    "\n",
    "        # Pour faire step, reset, state pour cette implementation\n",
    "        self.env = RacingCar(track_name)\n",
    "        self.state_gym,_ = self.env.reset()\n",
    "        self.n_action = self.env.nb_state\n",
    "\n",
    "        # Décrit le model actuel\n",
    "        self.model = DQN(400,8,self.n_action)\n",
    "\n",
    "        # Décrit les transitions observées jusqu'à présent\n",
    "        self.memory = ReplayMemory(10000)\n",
    "\n",
    "        # Pour normaliser la Q table\n",
    "        self.discount_factor = 0.9\n",
    "\n",
    "    def state(self):\n",
    "        \"\"\" On définit un état comme étant un batch de taille 1 ou None\"\"\"\n",
    "        if (self.state_gym == None or self.done) :\n",
    "            return None\n",
    "        else :\n",
    "            arr = np.array(self.env.get_state())\n",
    "            arr = arr / max(MAX_SPEED, self.env.max_dist_wall) #To normalize the array\n",
    "            return torch.tensor([arr],dtype=torch.float)\n",
    "\n",
    "    def show_state(self):\n",
    "        self.env.render(show_trajectory=True)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment\"\"\"\n",
    "        self.state_gym , _ = self.env.reset()\n",
    "        self.done = False\n",
    "    \n",
    "    def dist(state):\n",
    "        \"\"\"Calcule la longueur d'un plus court chemin entre state et goal (sous forme d'un flotant)\"\"\"\n",
    "        goal = torch.tensor([[11,3]],dtype=torch.float)\n",
    "        start = torch.tensor([[0,3]],dtype=torch.float)\n",
    "        if (torch.equal(state,start)):\n",
    "           return torch.tensor(13,dtype=torch.float)\n",
    "        else :\n",
    "           return torch.sum(torch.abs(state-goal))\n",
    "       \n",
    "    def step(self,action : torch.tensor) :\n",
    "        \"\"\" Fais un pas depuis l'état actuel via l'action donnée et renvoit la transition observéex\n",
    "            Une action est un tenseur contenant un seul scalaire \"\"\"\n",
    "        if (self.done):\n",
    "            raise(ValueError(\"Trying to move from a final state\"))\n",
    "\n",
    "        prev_state = self.state()\n",
    "\n",
    "        # do the step and update the new gym state\n",
    "        acc, turn = self.env.int_to_action(action.item())\n",
    "        if np.absolute(turn) > self.env.max_turn:\n",
    "            print(action.item(), (acc, turn))\n",
    "            \n",
    "        self.state_gym,reward,terminated,truncated,_ = self.env.step(action.item())\n",
    "        self.done = terminated or truncated\n",
    "\n",
    "        next_state = self.state()\n",
    "\n",
    "        reward_normalizer = max(self.env.max_speed, 100)\n",
    "        qtable_normalizer = 1/(1-self.discount_factor)\n",
    "        reward = torch.tensor(reward/(reward_normalizer*qtable_normalizer), dtype=torch.float).reshape((1,1))\n",
    "        action = torch.tensor(action.item()).reshape((1,1))\n",
    "\n",
    "        transition = Transition(prev_state, action, next_state , reward)\n",
    "        return transition\n",
    "    \n",
    "    def policy(self):\n",
    "        if (self.done):\n",
    "            raise(ValueError(\"Trying to predict a move from a final state\"))\n",
    "        return self.model(self.state()).max(1).indices.reshape((1,1))\n",
    "    \n",
    "    def random_action(self) -> torch.tensor :\n",
    "        if (self.done):\n",
    "            raise(ValueError(\"Trying to sample a move from a final state\"))\n",
    "        action = rd.randint(0,self.n_action-1)\n",
    "        return torch.tensor(action).reshape((1,1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(env : Env,optimizer,criterion,batch_size,discount_factor):\n",
    "    if (len(env.memory) < batch_size) :\n",
    "        return \n",
    "\n",
    "    # A list of batch_size transtions\n",
    "    transition = env.memory.sample(batch_size)\n",
    "\n",
    "    # A tuple with four coordinates : \n",
    "    # state -> a batch of size batch_size of states \n",
    "    # action -> a batch of size batch_size of actions\n",
    "    # ect\n",
    "    batch = Transition(*zip(*transition))\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Batch of size batch_size of the Qvalue predicted by our current model, for the state and action of a transtion\n",
    "    predicted = env.model(state_batch).gather(1,action_batch)\n",
    "\n",
    "    next_state_value = torch.zeros((batch_size,1))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool )\n",
    "    if non_final_mask.any():\n",
    "        non_final_next_state = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        with torch.no_grad():\n",
    "            next_state_value[non_final_mask] = env.model(non_final_next_state).max(1).values.unsqueeze(1)\n",
    "\n",
    "    expected = reward_batch + (discount_factor * next_state_value)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(predicted,expected)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(env.model.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(env):\n",
    "    #env.model.load(filename)\n",
    "    filename =  \"saved_model/\"  + datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size = 40\n",
    "    epochs = 5000\n",
    "    max_episode_duration = 1000\n",
    "    epsilon_max = 1\n",
    "    epsilon_min = 0.01\n",
    "    epsilon_decay = 30.\n",
    "    lr = 1e-4\n",
    "    discount_factor = 0.9\n",
    "    env.discount_factor = discount_factor\n",
    "    optimizer = optim.AdamW(env.model.parameters(), lr=lr, amsgrad=True)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    env.memory.clear()\n",
    "    reward_history = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        env.reset()\n",
    "        epsilon = epsilon_min + (epsilon_max-epsilon_min)*np.exp(-i/epsilon_decay)\n",
    "        it_counter = 0\n",
    "        reward = 0\n",
    "        while(not(env.done) and it_counter < max_episode_duration):\n",
    "            it_counter += 1\n",
    "            # Chose an action\n",
    "            if (rd.random() <= epsilon):\n",
    "                action = env.random_action()\n",
    "            else:\n",
    "                with torch.no_grad() :\n",
    "                    action = env.policy()\n",
    "\n",
    "            # Apply the transition and save it in memory\n",
    "            transition = env.step(action)\n",
    "            reward += (transition.reward).item()\n",
    "            env.memory.push(transition)\n",
    "            \n",
    "            # Optimize the model according to the observed reward\n",
    "            #optimizer_one_by_one(env,transition,optimizer,criterion,discount_factor)\n",
    "            optimize(env,optimizer,criterion,batch_size,discount_factor)\n",
    "            #optimize_weak(env,optimizer,criterion,batch_size,discount_factor)\n",
    "            #env.show_state()\n",
    "            if (i%100 == 0) :\n",
    "                env.show_state()\n",
    "\n",
    "        # if (i%100 == 0) :\n",
    "        #         env.show_state()\n",
    "        normalizer = 1\n",
    "        reward_history.append(reward*normalizer)\n",
    "\n",
    "    window = 20*[1/20]\n",
    "    reward_history = np.convolve(reward_history, window ,  mode=\"valid\")\n",
    "    plt.plot(reward_history)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myenv = Env(\"../tracks/02.png\")\n",
    "print(myenv.n_action)\n",
    "training(myenv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    myenv.reset()\n",
    "    while(myenv.done == False) :\n",
    "        myenv.step(myenv.policy())\n",
    "        myenv.show_state()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
