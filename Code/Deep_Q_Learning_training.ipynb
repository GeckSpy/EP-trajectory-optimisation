{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tracks: 86\n",
      "[('../tracks/png/12.png', '12'), ('../tracks/png/44.png', '44'), ('../tracks/png/41.png', '41'), ('../tracks/png/08.png', '08'), ('../tracks/png/90.png', '90'), ('../tracks/png/50.png', '50'), ('../tracks/png/46.png', '46'), ('../tracks/png/70.png', '70'), ('../tracks/png/07.png', '07'), ('../tracks/png/78.png', '78'), ('../tracks/png/57.png', '57'), ('../tracks/png/48.png', '48'), ('../tracks/png/30.png', '30'), ('../tracks/png/93.png', '93'), ('../tracks/png/61.png', '61'), ('../tracks/png/80.png', '80'), ('../tracks/png/76.png', '76'), ('../tracks/png/16.png', '16'), ('../tracks/png/89.png', '89'), ('../tracks/png/56.png', '56'), ('../tracks/png/04.png', '04'), ('../tracks/png/15.png', '15'), ('../tracks/png/28.png', '28'), ('../tracks/png/73.png', '73'), ('../tracks/png/58.png', '58'), ('../tracks/png/20.png', '20'), ('../tracks/png/09.png', '09'), ('../tracks/png/75.png', '75'), ('../tracks/png/11.png', '11'), ('../tracks/png/14.png', '14'), ('../tracks/png/82.png', '82'), ('../tracks/png/32.png', '32'), ('../tracks/png/72.png', '72'), ('../tracks/png/34.png', '34'), ('../tracks/png/06.png', '06'), ('../tracks/png/59.png', '59'), ('../tracks/png/77.png', '77'), ('../tracks/png/31.png', '31'), ('../tracks/png/54.png', '54'), ('../tracks/png/17.png', '17'), ('../tracks/png/87.png', '87'), ('../tracks/png/86.png', '86'), ('../tracks/png/39.png', '39'), ('../tracks/png/91.png', '91'), ('../tracks/png/67.png', '67'), ('../tracks/png/42.png', '42'), ('../tracks/png/53.png', '53'), ('../tracks/png/05.png', '05'), ('../tracks/png/01.png', '01'), ('../tracks/png/35.png', '35'), ('../tracks/png/52.png', '52'), ('../tracks/png/60.png', '60'), ('../tracks/png/19.png', '19'), ('../tracks/png/02.png', '02'), ('../tracks/png/83.png', '83'), ('../tracks/png/81.png', '81'), ('../tracks/png/88.png', '88'), ('../tracks/png/38.png', '38'), ('../tracks/png/71.png', '71'), ('../tracks/png/25.png', '25'), ('../tracks/png/21.png', '21'), ('../tracks/png/36.png', '36'), ('../tracks/png/47.png', '47'), ('../tracks/png/24.png', '24'), ('../tracks/png/37.png', '37'), ('../tracks/png/55.png', '55'), ('../tracks/png/23.png', '23'), ('../tracks/png/63.png', '63'), ('../tracks/png/74.png', '74'), ('../tracks/png/51.png', '51'), ('../tracks/png/27.png', '27'), ('../tracks/png/66.png', '66'), ('../tracks/png/64.png', '64'), ('../tracks/png/22.png', '22'), ('../tracks/png/13.png', '13'), ('../tracks/png/43.png', '43'), ('../tracks/png/68.png', '68'), ('../tracks/png/33.png', '33'), ('../tracks/png/03.png', '03'), ('../tracks/png/69.png', '69'), ('../tracks/png/79.png', '79'), ('../tracks/png/26.png', '26'), ('../tracks/png/62.png', '62'), ('../tracks/png/18.png', '18'), ('../tracks/png/10.png', '10'), ('../tracks/png/45.png', '45')]\n",
      "0 ../tracks/png/12.png as 130 lines\n",
      "1 ../tracks/png/44.png as 97 lines\n",
      "2 ../tracks/png/41.png as 105 lines\n",
      "3 ../tracks/png/08.png as 72 lines\n",
      "4 ../tracks/png/90.png as 91 lines\n",
      "5 ../tracks/png/50.png as 95 lines\n",
      "6 ../tracks/png/46.png as 116 lines\n",
      "7 ../tracks/png/70.png as 132 lines\n",
      "8 ../tracks/png/07.png as 134 lines\n",
      "9 ../tracks/png/78.png as 106 lines\n",
      "10 ../tracks/png/57.png as 100 lines\n",
      "11 ../tracks/png/48.png as 114 lines\n",
      "12 ../tracks/png/30.png as 170 lines\n",
      "13 ../tracks/png/93.png as 93 lines\n",
      "14 ../tracks/png/61.png as 110 lines\n",
      "15 ../tracks/png/80.png as 102 lines\n",
      "16 ../tracks/png/76.png as 99 lines\n",
      "17 ../tracks/png/16.png as 147 lines\n",
      "18 ../tracks/png/89.png as 90 lines\n",
      "19 ../tracks/png/56.png as 123 lines\n",
      "20 ../tracks/png/04.png as 96 lines\n",
      "21 ../tracks/png/15.png as 106 lines\n",
      "22 ../tracks/png/28.png as 101 lines\n",
      "23 ../tracks/png/73.png as 119 lines\n",
      "24 ../tracks/png/58.png as 119 lines\n",
      "25 ../tracks/png/20.png as 108 lines\n",
      "26 ../tracks/png/09.png as 102 lines\n",
      "27 ../tracks/png/75.png as 97 lines\n",
      "28 ../tracks/png/11.png as 122 lines\n",
      "29 ../tracks/png/14.png as 117 lines\n",
      "30 ../tracks/png/82.png as 108 lines\n",
      "31 ../tracks/png/32.png as 109 lines\n",
      "32 ../tracks/png/72.png as 115 lines\n",
      "33 ../tracks/png/34.png as 106 lines\n",
      "34 ../tracks/png/06.png as 152 lines\n",
      "35 ../tracks/png/59.png as 109 lines\n",
      "36 ../tracks/png/77.png as 116 lines\n",
      "37 ../tracks/png/31.png as 104 lines\n",
      "38 ../tracks/png/54.png as 101 lines\n",
      "39 ../tracks/png/17.png as 95 lines\n",
      "40 ../tracks/png/87.png as 95 lines\n",
      "41 ../tracks/png/86.png as 85 lines\n",
      "42 ../tracks/png/39.png as 161 lines\n",
      "43 ../tracks/png/91.png as 107 lines\n",
      "44 ../tracks/png/67.png as 96 lines\n",
      "45 ../tracks/png/42.png as 93 lines\n",
      "46 ../tracks/png/53.png as 96 lines\n",
      "47 ../tracks/png/05.png as 122 lines\n",
      "48 ../tracks/png/01.png as 114 lines\n",
      "49 ../tracks/png/35.png as 94 lines\n",
      "50 ../tracks/png/52.png as 103 lines\n",
      "51 ../tracks/png/60.png as 124 lines\n",
      "52 ../tracks/png/19.png as 89 lines\n",
      "53 ../tracks/png/02.png as 132 lines\n",
      "54 ../tracks/png/83.png as 104 lines\n",
      "55 ../tracks/png/81.png as 118 lines\n",
      "56 ../tracks/png/88.png as 84 lines\n",
      "57 ../tracks/png/38.png as 169 lines\n",
      "58 ../tracks/png/71.png as 177 lines\n",
      "59 ../tracks/png/25.png as 77 lines\n",
      "60 ../tracks/png/21.png as 82 lines\n",
      "61 ../tracks/png/36.png as 103 lines\n",
      "62 ../tracks/png/47.png as 159 lines\n",
      "63 ../tracks/png/24.png as 122 lines\n",
      "64 ../tracks/png/37.png as 119 lines\n",
      "65 ../tracks/png/55.png as 105 lines\n",
      "66 ../tracks/png/23.png as 100 lines\n",
      "67 ../tracks/png/63.png as 118 lines\n",
      "68 ../tracks/png/74.png as 101 lines\n",
      "69 ../tracks/png/51.png as 92 lines\n",
      "70 ../tracks/png/27.png as 94 lines\n",
      "71 ../tracks/png/66.png as 107 lines\n",
      "72 ../tracks/png/64.png as 122 lines\n",
      "73 ../tracks/png/22.png as 95 lines\n",
      "74 ../tracks/png/13.png as 116 lines\n",
      "75 ../tracks/png/43.png as 95 lines\n",
      "76 ../tracks/png/68.png as 113 lines\n",
      "77 ../tracks/png/33.png as 107 lines\n",
      "78 ../tracks/png/03.png as 107 lines\n",
      "79 ../tracks/png/69.png as 108 lines\n",
      "80 ../tracks/png/79.png as 120 lines\n",
      "81 ../tracks/png/26.png as 100 lines\n",
      "82 ../tracks/png/62.png as 139 lines\n",
      "83 ../tracks/png/18.png as 138 lines\n",
      "84 ../tracks/png/10.png as 78 lines\n",
      "85 ../tracks/png/45.png as 101 lines\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "# Essentials for training and env\n",
    "\n",
    "# Car env\n",
    "from env_06 import RacingCar, PATHS, TRACKS_FOLDER, TRACKS, MAX_SPEED\n",
    "\n",
    "# For neural networks\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# For math computations\n",
    "import numpy as np\n",
    "\n",
    "# For random\n",
    "import random as rd\n",
    "\n",
    "# For envs\n",
    "import gymnasium\n",
    "\n",
    "# For time limit\n",
    "import time\n",
    "\n",
    "# Structure to save transitions \n",
    "from collections import namedtuple , deque\n",
    "Transition = namedtuple(\"Transition\",[\"state\",\"action\",\"next_state\",\"reward\"])\n",
    "\n",
    "# For plots\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "# For saving files\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "# for model vizualisation \n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self,maxlen : int):\n",
    "        self.memory_ = deque(maxlen=maxlen)\n",
    "\n",
    "    def push(self,x : Transition):\n",
    "        \"\"\"Add a new transition to the memory\"\"\"\n",
    "        self.memory_.append(x)\n",
    "\n",
    "    def sample(self,batch_size : int) -> list[Transition]:\n",
    "        \"\"\"Sample a subset of size batch_size uniformly among the subsets of size batch_size of the memory. \n",
    "           We assume that such subset exists.\n",
    "        \"\"\"\n",
    "        return rd.sample(self.memory_,batch_size)\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear the memory\"\"\"\n",
    "        return self.memory_.clear()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self,layer_size,state_size,action_n):\n",
    "        super(DQN,self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_size,layer_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(layer_size,layer_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(layer_size,layer_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(layer_size,action_n),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def save(self,filename : str = None):\n",
    "        \"\"\"Save model's parameters in the given file\"\"\"\n",
    "        if (filename == None):\n",
    "            filename = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "        torch.save(self.state_dict(),filename)\n",
    "\n",
    "    def load(self,filename : str):\n",
    "        \"\"\"Load parameters stored in the given file\"\"\"\n",
    "        self.load_state_dict(torch.load(filename, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self):\n",
    "        self.done = False\n",
    "\n",
    "        # Underlying environment\n",
    "        self.env = RacingCar()\n",
    "        self.state_gym,_ = self.env.reset(TRACKS[0])\n",
    "        self.n_action = self.env.nb_state\n",
    "\n",
    "        # Current model estimating the Q-function\n",
    "        self.model = DQN(400,8,self.n_action)\n",
    "\n",
    "        # Transition history\n",
    "        self.memory = ReplayMemory(10000)\n",
    "\n",
    "        # To normalize the Q-function later, in the reward function\n",
    "        self.discount_factor = 0.9\n",
    "\n",
    "        # Number of tracks allowed\n",
    "        self.track_for_training = int(len(TRACKS)*0.8)\n",
    "\n",
    "    def state(self):\n",
    "        \"\"\"A state is either a batch of size 1 or None\"\"\"\n",
    "        if (self.state_gym == None or self.done) :\n",
    "            return None\n",
    "        else :\n",
    "            arr = np.array(self.env.get_state())\n",
    "            arr = arr / max(MAX_SPEED, self.env.max_dist_wall) #To normalize the array\n",
    "            return torch.tensor([arr],dtype=torch.float)\n",
    "\n",
    "    def show_state(self):\n",
    "        \"\"\"plot the current trajectory\"\"\"\n",
    "        self.env.render(show_trajectory=True)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment\"\"\"\n",
    "        rd_track = rd.randint(0, self.track_for_training -1)\n",
    "        self.state_gym , _ = self.env.reset(TRACKS[rd_track])\n",
    "        self.done = False\n",
    "       \n",
    "    def step(self,action : torch.tensor) :\n",
    "        \"\"\"Move from the current state to the next state thanks to the action, observing a reward\"\"\"\n",
    "        if (self.done):\n",
    "            raise(ValueError(\"Trying to move from a final state\"))\n",
    "\n",
    "        prev_state = self.state()\n",
    "\n",
    "        # Do the step and update the new gym state\n",
    "        acc, turn = self.env.int_to_action(action.item())\n",
    "        if np.absolute(turn) > self.env.max_turn:\n",
    "            print(action.item(), (acc, turn))\n",
    "            \n",
    "        self.state_gym,reward,terminated,truncated,_ = self.env.step(action.item())\n",
    "        self.done = terminated or truncated\n",
    "\n",
    "        next_state = self.state()\n",
    "\n",
    "        reward_normalizer = self.env.reward_max\n",
    "        qtable_normalizer = 1/(1-self.discount_factor)\n",
    "        # We normalize the reward\n",
    "        reward = torch.tensor(reward/(reward_normalizer*qtable_normalizer), dtype=torch.float).reshape((1,1))\n",
    "        action = torch.tensor(action.item()).reshape((1,1))\n",
    "\n",
    "        transition = Transition(prev_state, action, next_state , reward)\n",
    "        return transition\n",
    "    \n",
    "    def policy(self):\n",
    "        \"\"\"Predict the best action according to our current model\"\"\"\n",
    "        if (self.done):\n",
    "            raise(ValueError(\"Trying to predict a move from a final state\"))\n",
    "        return self.model(self.state()).max(1).indices.reshape((1,1))\n",
    "    \n",
    "    def random_action(self) -> torch.tensor :\n",
    "        \"\"\"Select a random action\"\"\"\n",
    "        if (self.done):\n",
    "            raise(ValueError(\"Trying to sample a move from a final state\"))\n",
    "        action = rd.randint(0,self.n_action-1)\n",
    "        return torch.tensor(action).reshape((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(env : Env,optimizer,criterion,batch_size,discount_factor):\n",
    "    \"\"\"Optimize the model according to batch_size random transitions observed in the past\"\"\"\n",
    "    # We optimize only if have >= batch_size transition in our replay buffer\n",
    "    if (len(env.memory) < batch_size) :\n",
    "        return \n",
    "\n",
    "    # A list of batch_size transtions\n",
    "    transition = env.memory.sample(batch_size)\n",
    "\n",
    "    # A tuple with four coordinates : \n",
    "    # state -> a batch of size batch_size of states \n",
    "    # action -> a batch of size batch_size of actions\n",
    "    # reward -> a batch of size batch_size of rewards\n",
    "    # next_state -> a batch of size batch_size of the next states\n",
    "    batch = Transition(*zip(*transition))\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Batch of size batch_size of the Qvalue predicted by our current model, for the state and action of a transtion\n",
    "    predicted = env.model(state_batch).gather(1,action_batch)\n",
    "\n",
    "    next_state_value = torch.zeros((batch_size,1))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool )\n",
    "    if non_final_mask.any():\n",
    "        non_final_next_state = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        with torch.no_grad():\n",
    "            next_state_value[non_final_mask] = env.model(non_final_next_state).max(1).values.unsqueeze(1)\n",
    "\n",
    "    expected = reward_batch + (discount_factor * next_state_value)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(predicted,expected)\n",
    "    loss.backward()\n",
    "    # We bound the gradient by 100\n",
    "    torch.nn.utils.clip_grad_value_(env.model.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_policy_time(env) :\n",
    "    \"\"\"Measure the time needed by the model to predict one action\"\"\"\n",
    "    env.reset()\n",
    "    time_deb = time.perf_counter()\n",
    "    env.policy()\n",
    "    return time.perf_counter() - time_deb\n",
    "\n",
    "def measure_model_size(env):\n",
    "    \"\"\"Count the number of parameters of the underlying neural network\"\"\"\n",
    "    return sum(p.numel() for p in env.model.parameters())\n",
    "\n",
    "def evaluate_model_reward(env):\n",
    "    \"\"\"Compute the average reward obtained by the model on the tracks he has never seen before\"\"\"\n",
    "    sum = 0\n",
    "    max_step = 300\n",
    "    for i in range(env.track_for_training,len(TRACKS)) :\n",
    "        env.state_gym , _ = env.env.reset(TRACKS[i])\n",
    "        env.done = False\n",
    "        i = 0\n",
    "        while(i < max_step and not(env.done)) :\n",
    "            i+=1\n",
    "            transition = env.step( env.policy() )\n",
    "            sum += transition.reward.item()\n",
    "    return sum/( len(TRACKS) - env.track_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(lr=1e-4,epsilon_decay=30.,batch_size = 40,time_bound = 60*(1),track_budget=int(0.8*len(TRACKS))):\n",
    "    \"\"\"\n",
    "    We train a model initialised with random parameters using a Deep Q-Learning algorithm.\n",
    "    We allow to change 4 hyperparameters : \n",
    "    - lr : the learning rate \n",
    "    - epsilon_decay : measure the amount of random choices the model will do during the training\n",
    "    - time_bound : the time budget allowed for the training\n",
    "    - track_budget : the number tracks allowed for the training\n",
    "    We output a dictionnary containing informations on the training.\n",
    "    \"\"\"\n",
    "    env = Env()\n",
    "    filename =  \"saved_model/\"  + datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    # Create the folder \"saved_model\" where the model's parameters will be saved\n",
    "    if not os.path.exists(\"saved_model\"):\n",
    "        os.makedirs(\"saved_model\")\n",
    "\n",
    "    # track budget for training\n",
    "    env.track_for_training = track_budget\n",
    "\n",
    "    # Hyperparameters\n",
    "    #batch_size = 40\n",
    "    epochs = 5000\n",
    "    max_episode_duration = 1000 * 1/env.env.time\n",
    "    epsilon_max = 1\n",
    "    epsilon_min = 0.01\n",
    "    #epsilon_decay = 30.\n",
    "    #lr = 1e-4\n",
    "    discount_factor = 0.9\n",
    "    env.discount_factor = discount_factor\n",
    "    optimizer = optim.AdamW(env.model.parameters(), lr=lr, amsgrad=True)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    env.memory.clear()\n",
    "\n",
    "    reward_history = []\n",
    "    reward_time = []\n",
    "    volatility_history = []\n",
    "    volatility_time = []\n",
    "\n",
    "    #time_bound = 60*(1)\n",
    "    time_start = time.perf_counter()\n",
    "    i = 0\n",
    "    while ( (time.perf_counter() - time_start <= time_bound)  ):\n",
    "        i += 1\n",
    "        env.reset()\n",
    "        epsilon = epsilon_min + (epsilon_max-epsilon_min)*np.exp(-i/epsilon_decay)\n",
    "        it_counter = 0\n",
    "        reward = 0\n",
    "        while(not(env.done) and it_counter < max_episode_duration):\n",
    "            it_counter += 1\n",
    "            # Chose an action\n",
    "            if (rd.random() <= epsilon):\n",
    "                action = env.random_action()\n",
    "            else:\n",
    "                with torch.no_grad() :\n",
    "                    action = env.policy()\n",
    "            # Apply the transition and save it in the memory\n",
    "            transition = env.step(action)\n",
    "            reward += (transition.reward).item()\n",
    "            env.memory.push(transition)\n",
    "            # Optimize the model according to batch_size random transitions\n",
    "            optimize(env,optimizer,criterion,batch_size,discount_factor)\n",
    "    \n",
    "        # Stats about the training\n",
    "        second = (int(time.perf_counter() - time_start)) % 60\n",
    "        minute = (int(time.perf_counter() - time_start)) //60\n",
    "        window_len = 30\n",
    "        iteration_time = time.perf_counter() - time_start\n",
    "        # We save the model every 5 minutes\n",
    "        if (minute%5 == 0 and minute > 3) :\n",
    "            env.model.save(filename)\n",
    "        reward_time.append( iteration_time  )\n",
    "        volatility_time.append(iteration_time)\n",
    "        reward_history.append(reward)\n",
    "        last_window = reward_history[-window_len:]\n",
    "        volatility_history.append(np.std(last_window))\n",
    "    \n",
    "    res = {}\n",
    "    res[\"training_time\"] = time_bound\n",
    "    res[\"track_number\"] = track_budget\n",
    "    res[\"global_volatility\"] = np.std(reward_history)\n",
    "    res[\"model_size\"] = measure_model_size(env)\n",
    "    res[\"policy_time\"] = measure_policy_time(env)\n",
    "    res[\"policy_score\"] = evaluate_model_reward(env)\n",
    "    res[\"reward_history\"] = reward_history\n",
    "    res[\"reward_time\"] = reward_time \n",
    "    res[\"volatility_history\"] = volatility_history\n",
    "    res[\"volatility_time\"] = volatility_time \n",
    "    res[\"DQN_model_param\"] = filename\n",
    "    res[\"DQN_model_param_is_saved\"] = False\n",
    "    res[\"learning rate\"] = lr \n",
    "    res[\"batch size\"] = batch_size\n",
    "    res[\"epsilon decay\"] = epsilon_decay\n",
    "\n",
    "    # we save the model if we trained it for more that 3 minutes\n",
    "    minute = (int(time.perf_counter() - time_start)) //60\n",
    "    if (minute > 3) :\n",
    "         env.model.save(filename)\n",
    "         res[\"DQN_model_param_is_saved\"] = True\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playing(env):\n",
    "    \"\"\"Observe how the model behave on all tracks\"\"\"\n",
    "    sum = 0\n",
    "    max_step = 300\n",
    "    mini = 0\n",
    "    for i in range(mini,len(TRACKS)) :\n",
    "        env.state_gym , _ = env.env.reset(TRACKS[i])\n",
    "        env.done = False\n",
    "        j = 0\n",
    "        while(j < max_step and not(env.done)) :\n",
    "            j+=1\n",
    "            transition = env.step( env.policy() )\n",
    "            sum += transition.reward.item()\n",
    "            env.show_state()\n",
    "    return sum/( len(TRACKS) - mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part1_data():\n",
    "    \"\"\"Compute the data to compare Deep Q-Learning with Genetic algorithms\"\"\"\n",
    "    training_times_min = [10,40,60]\n",
    "    training_times_sec = [60*i for i in training_times_min]\n",
    "    track_limit = [8,40, int(0.8*len(TRACKS)) ]\n",
    "    # We check if we have enough tracks to do the training\n",
    "    if (len(TRACKS) < max(track_limit)) :\n",
    "        raise(ValueError(\"not enough tracks\"))\n",
    "    # Create the folder associated with the training\n",
    "    folder_name =  \"run_part1_\"+ datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    os.makedirs(folder_name)\n",
    "    # Train the model\n",
    "    for time_bound in training_times_sec :\n",
    "        for track_budget in track_limit :\n",
    "            json_object = json.dumps( training(time_bound=time_bound,track_budget=track_budget))\n",
    "            with open(folder_name + \"/\" + str(time_bound)+\"_\"+str(track_budget)+\".json\",\"w\") as f :\n",
    "                f.write(json_object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part2_data():\n",
    "    \"\"\"Compute the data to evaluate the dependence of the training on Hyperparameters (learning rate, batch size and epsilon decay)\"\"\"\n",
    "    training_time = 30*(60)\n",
    "    lr_l = [1e-2,1e-3,1e-4,1e-5,1e-6]\n",
    "    batch_size_l = [10,30,50,80,120]\n",
    "    epsilon_decay_l = [10.,30.,50.,120.,200.]\n",
    "    track_budget = int(0.8*len(TRACKS))\n",
    "\n",
    "    # Create the folder associated with the training\n",
    "    folder_name =  \"run_part2_\"+ datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "    # For learning rate\n",
    "    for lr in lr_l :\n",
    "        json_object = json.dumps( training(lr=lr, track_budget=track_budget , time_bound=training_time ))\n",
    "        with open(folder_name + \"/\" + \"lr\"+\"_\"+str(lr)+\".json\",\"w\") as f :\n",
    "            f.write(json_object)\n",
    "\n",
    "     # For batch size\n",
    "    for batch_size in batch_size_l :\n",
    "        json_object = json.dumps( training(batch_size=batch_size, track_budget=track_budget , time_bound=training_time ))\n",
    "        with open(folder_name + \"/\" + \"batch_size\"+\"_\"+str(batch_size)+\".json\",\"w\") as f :\n",
    "            f.write(json_object)\n",
    "\n",
    "    # For epsilon decay\n",
    "    for epsilon_decay in epsilon_decay_l :\n",
    "        json_object = json.dumps( training(epsilon_decay=epsilon_decay, track_budget=track_budget , time_bound=training_time ))\n",
    "        with open(folder_name + \"/\" + \"epsilon_decay\"+\"_\"+str(epsilon_decay)+\".json\",\"w\") as f :\n",
    "            f.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part3_data():\n",
    "    \"Compute the data associated with a training of six hours\"\n",
    "    training_times = (60)*(60)*6\n",
    "    track_limit = int(0.8*len(TRACKS))\n",
    "    # Create the folder associated with the training\n",
    "    folder_name =  \"run_part3_\"+ datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    os.makedirs(folder_name)\n",
    "    # Train the model\n",
    "    json_object = json.dumps( training(time_bound=training_times,track_budget=track_limit, lr=1e-5 , epsilon_decay=500.))\n",
    "    with open(folder_name + \"/\" + \"long training\"+\".json\",\"w\") as f :\n",
    "        f.write(json_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
