{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium\n",
    "\n",
    "import random \n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple , deque\n",
    "Transition = namedtuple(\"Transition\",[\"state\",\"action\",\"next_state\",\"reward\",\"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"To copy construct from a tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self,maxlen : int):\n",
    "        self.memory = deque(maxlen=maxlen)\n",
    "\n",
    "    def push(self,x : Transition):\n",
    "        self.memory.append(x)\n",
    "\n",
    "    def sample(self,batch_size : int) -> list[Transition]:\n",
    "        return random.sample(self.memory,batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN,self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(2,24),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(24,24),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # nn.Linear(64,64),\n",
    "            # nn.ReLU(),\n",
    "\n",
    "            nn.Linear(24,4),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def save(self,filename : str = None):\n",
    "        if (filename == None):\n",
    "            filename = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "        torch.save(self.state_dict(),filename)\n",
    "\n",
    "    def load(self,filename : str):\n",
    "        self.load_state_dict(torch.load(filename, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self,env_name):\n",
    "        self.env = gymnasium.make(env_name,render_mode=\"rgb_array\")\n",
    "        self.state_gym,_ = self.env.reset()\n",
    "        self.model = DQN()\n",
    "        self.replay = ReplayMemory(10000)\n",
    "\n",
    "    def show_state(self):\n",
    "        img = self.env.render()\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        display.clear_output(wait=True)\n",
    "        plt.show()\n",
    "\n",
    "    def policy(self):\n",
    "        prediction = self.model(self.state())\n",
    "        return torch.max(prediction,0).indices\n",
    "    \n",
    "    # To build \n",
    "    def state(self) -> torch.tensor:\n",
    "        if (self.state_gym == None) :\n",
    "            return torch.tensor([-1,-1],dtype=torch.float)\n",
    "        y = self.state_gym//12\n",
    "        x = self.state_gym % 12\n",
    "        return torch.tensor([x,y],dtype=torch.float)\n",
    "    \n",
    "    # To build\n",
    "    def reset(self):\n",
    "        self.state_gym,_ = self.env.reset()\n",
    "\n",
    "     # To build\n",
    "    def random_action(self) -> torch.tensor:\n",
    "        gym_action = self.env.action_space.sample()\n",
    "        return torch.tensor(gym_action)\n",
    "    \n",
    "    # \n",
    "    def dist(state):\n",
    "       goal = torch.tensor([11,3],dtype=float)\n",
    "       if (state[0] == torch.tensor(0) and state[1] == torch.tensor(3)) :\n",
    "           return torch.tensor(13)\n",
    "       else :\n",
    "           return torch.tensor( abs(state[0]-goal[0]) + abs(state[1]-goal[1]))\n",
    "    # Action : tensor ->  State : tensor , Reward : tensor , Done : bool \n",
    "    def step(self,action):\n",
    "        prev_state = self.state()\n",
    "        gym_next_state,reward,terminated,truncated,info = self.env.step(int(action))\n",
    "        self.state_gym = gym_next_state\n",
    "        done= terminated or truncated or (gym_next_state == 36)\n",
    "        next_state = self.state()\n",
    "        r = Env.dist(prev_state) - Env.dist(next_state) - 2\n",
    "        if (truncated or (gym_next_state == 36)) :\n",
    "            r -= 100\n",
    "        if (terminated) :\n",
    "            r += 100\n",
    "\n",
    "        self.replay.push(Transition(state=prev_state, action=action, next_state= next_state , reward=torch.tensor(r) , done = done ))\n",
    "\n",
    "        return next_state, torch.tensor(r) , done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env : Env, try_n : int = 10, maxlen : int = 100):\n",
    "    s = .0\n",
    "    for i in range(try_n):\n",
    "        env.reset()\n",
    "        for j in range(maxlen):\n",
    "            obs,r,done = env.step(env.policy())\n",
    "            s += r.item()\n",
    "            if (done) : \n",
    "                break\n",
    "    return s/try_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_random(env):\n",
    "    for i in range(100):\n",
    "        env.reset()\n",
    "        while(True):\n",
    "            state,reward,done = env.step(env.random_action())\n",
    "            env.show_state()\n",
    "            if (done):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_policy(env):\n",
    "    for i in range(100):\n",
    "        env.reset()\n",
    "        while(True):\n",
    "            action = env.policy()\n",
    "            state,reward,done = env.step(env.policy())\n",
    "            env.show_state()\n",
    "            if (done):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(env : Env,optimizer,criterion,batch_size,discount_factor):\n",
    "    if (len(env.replay) < batch_size) :\n",
    "        return \n",
    "    \n",
    "    batch = env.replay.sample(batch_size)\n",
    "    expected = torch.empty((batch_size),dtype=torch.float)\n",
    "    predicted = torch.empty((batch_size),dtype=torch.float)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        expected[i] = env.model(batch[i].state)[ int(batch[i].action)]\n",
    "        if (not(batch[i].done)):\n",
    "            with torch.no_grad() :\n",
    "                predicted[i] = batch[i].reward + discount_factor * env.model(batch[i].next_state).max()\n",
    "        else:\n",
    "            predicted[i] = batch[i].reward\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(predicted,expected)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(env.model.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(env):\n",
    "    epochs = 1000\n",
    "    batch_size = 128\n",
    "\n",
    "    epsilon = 1\n",
    "    epsilon_max = 1\n",
    "    epsilon_min = 0.05\n",
    "    epsilon_decay = 1000.0\n",
    "\n",
    "    lr = 1e-4\n",
    "    discount_factor = 0.99\n",
    "\n",
    "    optimizer = optim.AdamW(env.model.parameters(), lr=lr, amsgrad=True)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    for i in range(epochs):\n",
    "        env.reset()\n",
    "        state = env.state()\n",
    "        epsilon = epsilon_min + (epsilon_max-epsilon_min)*np.exp(-i/epsilon_decay)\n",
    "        while(True):\n",
    "            if (random.random() <= epsilon ):\n",
    "                action = env.random_action()\n",
    "            else :\n",
    "                with torch.no_grad() :\n",
    "                    action = env.policy()\n",
    "            next_state,reward,done = env.step(action)\n",
    "\n",
    "            # predicted_value = env.model(state)[action]\n",
    "            # with torch.no_grad() :\n",
    "            #     expected_value = reward + discount_factor * env.model(next_state).max()\n",
    "\n",
    "            # optimizer.zero_grad()\n",
    "            # loss = criterion(predicted_value,expected_value)\n",
    "            # loss.backward()\n",
    "            # torch.nn.utils.clip_grad_value_(env.model.parameters(), 100)\n",
    "            # optimizer.step()\n",
    "\n",
    "            optimize(env,optimizer,criterion,batch_size,discount_factor)\n",
    "\n",
    "            if (done) :\n",
    "                break \n",
    "            else :\n",
    "                #env.show_state()\n",
    "                state = next_state\n",
    "\n",
    "    env.model.save(\"saved_model/model_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m game_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCliffWalking-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m env\u001b[38;5;241m=\u001b[39m Env(game_name)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[69], line 38\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m     26\u001b[0m next_state,reward,done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# predicted_value = env.model(state)[action]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# with torch.no_grad() :\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#     expected_value = reward + discount_factor * env.model(next_state).max()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# torch.nn.utils.clip_grad_value_(env.model.parameters(), 100)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# optimizer.step()\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdiscount_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (done) :\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m \n",
      "Cell \u001b[0;32mIn[68], line 19\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(env, optimizer, criterion, batch_size, discount_factor)\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predicted,expected)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_value_(env\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "game_name = \"CliffWalking-v0\"\n",
    "env= Env(game_name)\n",
    "training(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-201.0\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
